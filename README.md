âš¡ BackTesting Engine
Quantitative Strategy Backtesting Framework with Cloud-Native Data Pipeline
ğŸ“– Project Overview
BackTesting Engine is a high-performance, config-driven quantitative backtesting framework designed to evaluate trading strategies at scale. It executes grid search across 64+ strategy parameter combinations using batched multiprocessing with a shared-nothing architecture.

The system transitioned from a naive crossover approach to a Regime-Aware Strategy, transforming a -$39,000 baseline loss into a potential +$17,800 profit through advanced risk management, volatility filtering, and momentum validation.

The system ingests live and historical BTCUSDT 15-minute candle data from the Binance API through a fully automated, event-driven AWS pipeline â€” using Lambda, EventBridge, and S3 â€” and feeds it into a Dockerized backtesting engine for reproducible, deterministic experimentation.

ğŸ—ï¸ System Architecture
The system follows a two-stage architecture: an automated cloud data ingestion pipeline, and a local Dockerized backtesting engine that consumes the collected data.

High-Level Architecture Flow
mermaid
graph TD
    classDef cloud fill:#e3f2fd,stroke:#0d47a1,stroke-width:2px
    classDef aws fill:#fff3e0,stroke:#e65100,stroke-width:2px
    classDef engine fill:#e8f5e9,stroke:#1b5e20,stroke-width:2px
    classDef output fill:#f3e5f5,stroke:#4a148c,stroke-width:2px
    classDef data fill:#fce4ec,stroke:#b71c1c,stroke-width:2px
    subgraph Cloud Pipeline
        EB["â° EventBridge Scheduler"]:::aws
        Lambda["âš™ï¸ AWS Lambda"]:::aws
        Binance["ğŸ“¡ Binance API"]:::cloud
        S3["ğŸª£ S3 Bucket"]:::aws
    end
    subgraph Local Machine
        Sync["ğŸ”„ AWS CLI Sync"]:::data
        Scripts["ğŸ› ï¸ Data Processing Scripts"]:::data
        Parquet["ğŸ“¦ Parquet Dataset"]:::data
    end
    subgraph Dockerized Engine
        Config["ğŸ“‹ YAML Config"]:::engine
        Factory["ğŸ­ Strategy Factory"]:::engine
        JobBuilder["ğŸ”§ Job Builder"]:::engine
        BatchRunner["âš¡ Batch Runner"]:::engine
        Workers["ğŸ‘· Parallel Workers"]:::engine
        BTE["ğŸ§  Backtesting Engine"]:::engine
    end
    subgraph Reporting
        JSON["ğŸ“„ JSON Results"]:::output
        CSV["ğŸ“Š Summary CSV"]:::output
        Plots["ğŸ“ˆ Equity & Drawdown Plots"]:::output
    end
    EB -->|"Cron Trigger"| Lambda
    Lambda -->|"Fetch 15m Candles"| Binance
    Binance -->|"OHLCV Data"| Lambda
    Lambda -->|"Store CSV"| S3
    S3 -->|"aws s3 sync"| Sync
    Sync --> Scripts
    Scripts -->|"Merge + Clean"| Parquet
    Parquet --> Config
    Config --> Factory
    Factory --> JobBuilder
    JobBuilder --> BatchRunner
    BatchRunner -->|"Multiprocessing Pool"| Workers
    Workers --> BTE
    BTE --> JSON
    BTE --> CSV
    BTE --> Plots
ğŸ“Š Results & Performance
The engine's primary objective was to optimize a standard Moving Average crossover into a robust, tradeable strategy.

The PnL Turnaround
Original baseline strategies often yielded significant losses due to "choppy" markets. By implementing Regime Filtering and Dynamic Stop Losses, we achieved a major performance shift:

Metric	Naive Strategy (Baseline)	Optimized Regime-Aware
Total PnL	-$39,120	+$17,855
Win Rate	21.0%	28.3%
Max Drawdown	-$41,881	-$11,406
Stop Loss	Fixed 1.0%	Dynamic 2.5x ATR
Top Performing Strategy (Summary)
Data extracted from latest 64-run grid search (
results/summary.csv
)

Strategy ID	Short/Long	Total PnL	Max Drawdown	PnL/DD Ratio
ma_25_110	25 / 110	$17,855.17	-$11,406	1.56
ma_25_120	25 / 120	$13,130.36	-$9,846	1.33
ma_20_110	20 / 110	$12,012.50	-$14,460	0.83
âš™ï¸ Strategy Logic: "The Regime SNIPER"
The strategy doesn't just look for crosses; it validates them against the current market "regime" to avoid false signals in flat markets.

1. The 4-Stage Entry Filter
Before a BUY signal is issued, the market must pass four strict tests:

EMA 200 Slope (The Trend): Today's EMA must be higher than 10 candles ago (Ensures a real uptrend).
ATR Volatility Filter (The Wake-Up): Today's range (ATR) must be > 80% of the 20-period average (Market must be "awake").
RSI Momentum (Hype Meter): RSI-14 must be between 50 and 75 (Strong pulse, not yet overbought).
Strict Crossover: Fast MA must cross above Slow MA on the current candle, not just be above it.
2. The Dynamic Safety Net (ATR Stop Loss)
Instead of a fixed percentage, the strategy uses Average True Range (ATR) to set volatility-adjusted stops: Stop Price = Entry Price - (2.5 * ATR) This allows the trade room to breathe during high volatility and tightens up during steady trends.

â˜ï¸ Cloud Data Ingestion Pipeline
The data pipeline is fully serverless, designed for zero-maintenance continuous data collection.

Pipeline Flow
mermaid
sequenceDiagram
    participant EB as EventBridge
    participant Lambda as AWS Lambda
    participant Binance as Binance API
    participant S3 as S3 Bucket
    participant Local as Local Machine
    EB->>Lambda: Trigger every 15 minutes
    Lambda->>Binance: GET /api/v3/klines (BTCUSDT, 15m)
    Binance-->>Lambda: OHLCV candle data
    Lambda->>S3: PUT object (partitioned CSV)
    Note over S3: s3://project-backtesting-data/raw/symbol=BTCUSDT/
    Local->>S3: aws s3 sync
    S3-->>Local: Download new candle files
âš¡ Engineering & Optimization
High-Performance Runner
Shared-Nothing Multiprocessing: Each worker process is completely isolated, loading its own data to avoid GIL (Global Interpreter Lock) contention.
Batched Execution: Jobs are chunked into batches matching CPU core counts for 100% hardware utilization.
Runtime Reduction: Achieved ~65% reduction in total execution time compared to serial processing.
Data Efficiency
Parquet Adoption: Data loading speed increased by ~60% by switching from CSV to Snappy-compressed Parquet.
Year-Partitioning: Enables selective reading for specific year segments, slashing I/O overhead.
~65% Storage Reduction: Columnar Parquet format significantly reduces disk footprint compared to raw CSV.
ğŸ“ Project Structure
BackTesting_Engine/
â”œâ”€â”€ Engine/                          # Core backtesting engine
â”‚   â”œâ”€â”€ backtesting_engine.py        # Main event loop
â”‚   â”œâ”€â”€ data_loader.py               # Format-agnostic data loader
â”‚   â”œâ”€â”€ datafeed.py                  # Candle-by-candle iterator
â”‚   â”œâ”€â”€ execution.py                 # Slippage + commission simulator
â”‚   â”œâ”€â”€ portfolio.py                 # Position & risk management
â”‚   â””â”€â”€ metrics.py                   # PnL, win rate, drawdown
â”‚
â”œâ”€â”€ Strategies/                      # Trading strategy implementations
â”‚   â”œâ”€â”€ basic_strategy.py            # Abstract base strategy
â”‚   â””â”€â”€ ma_crossover.py             # MA Crossover with EMA regime filter
â”‚
â”œâ”€â”€ Runner/                          # Parallel execution framework
â”‚   â”œâ”€â”€ config_loader.py             # YAML config parser
â”‚   â”œâ”€â”€ strategy_factory.py          # Parameter grid generator
â”‚   â”œâ”€â”€ job_builder.py               # Isolated engine builder per job
â”‚   â”œâ”€â”€ batch_runner.py              # Batched multiprocessing dispatcher
â”‚   â”œâ”€â”€ parallel_runner.py           # Direct parallel runner
â”‚   â””â”€â”€ worker.py                    # Shared-nothing worker process
â”‚
â”œâ”€â”€ reporting/                       # Output & analytics pipeline
â”‚   â”œâ”€â”€ result_writer.py             # JSON + CSV result persistence
â”‚   â”œâ”€â”€ plots.py                     # Equity & drawdown curve generator
â”‚   â””â”€â”€ analytics.py                 # Post-run enrichment & ranking
â”‚
â”œâ”€â”€ Scripts/                         # Data processing utilities
â”‚   â”œâ”€â”€ build_final_dataset.py       # Bootstrap + live data merger
â”‚   â”œâ”€â”€ bootstrap_merge.py           # Historical data format handler
â”‚   â”œâ”€â”€ csv_to_parquet.py            # CSV â†’ Parquet converter
â”‚   â””â”€â”€ csv_to_partitioned_parquet.py# Year-partitioned Parquet builder
â”‚
â”œâ”€â”€ tools/
â”‚   â””â”€â”€ generate_report.py           # Clean + run + report pipeline
â”‚
â”œâ”€â”€ config/
â”‚   â””â”€â”€ experiment.yaml              # Strategy grid & engine configuration
â”‚
â”œâ”€â”€ data/                            # Market data (Raw S3 + Processed Parquet)
â”œâ”€â”€ results/                         # Ranked CSVs & Equity Curve PNGs
â”œâ”€â”€ Dockerfile                       # Container specification
â”œâ”€â”€ main.py                          # Entry point
â””â”€â”€ requirements.txt                 # Python dependencies
ğŸš€ Getting Started
1. Sync Market Data from S3
bash
aws s3 sync s3://project-backtesting-data/raw/ ./data/raw/
2. Build & Merge Dataset
bash
python Scripts/build_final_dataset.py
3. Run Backtesting Engine
bash
python main.py
4. Check Results
Check 
results/summary.csv
 for the top-ranked strategy variants.
Visual equity and drawdown curves can be found in results/plots/.
ğŸ“„ License
This project is licensed under the MIT License.

â­ Acknowledgements
BackTesting Engine was built to explore quantitative strategy evaluation, cloud-native data pipelines, high-performance parallel computation, and reproducible research environments using modern Python and AWS infrastructure.

